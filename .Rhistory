library(readr)
test <- read_csv("data/3-sentiment/test.csv")
View(test)
library(readr)
test <- read_csv("data/3-sentiment/test.csv")
View(test)
library(readr)
train <- read_csv("data/3-sentiment/train.csv")
View(train)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
install.packages("knitr")
knitr::opts_chunk$set(echo = TRUE)
install.packages("rmarkdown")
knitr::opts_chunk$set(echo = TRUE)
# here goes a list of recommended libraries,
# though you may install other ones if they are needed
library(tidytext)
install.packages(tidytext)
install.packages("tidytext")
# here goes a list of recommended libraries,
# though you may install other ones if they are needed
library(tidytext)
library(readr)
library(dplyr)
library(ggplot2)
install.packages("ggplot2")
getwd()
list.files(getwd())
list.files("data/3-sentiment")
test_path <- "data/3-sentiment/test.csv"
train_path <- "data/3-sentiment/train.csv"
stop_words <- read_file("stop_words.txt")
# https://stackoverflow.com/questions/27195912/why-does-strsplit-return-a-list
splitted_stop_words <- strsplit(stop_words, split='\n')
splitted_stop_words <- splitted_stop_words[[1]]
splitted_stop_words
train <-  read.csv(file = train_path, stringsAsFactors = FALSE)
test <-  read.csv(file = test_path, stringsAsFactors = FALSE)
# note the power functional features of R bring us!
tidy_text <- unnest_tokens(train, 'splitted', 'text', token="words") %>%
filter(!splitted %in% stop_words)
tidy_text %>% count(splitted,sort=TRUE)
# note the power functional features of R bring us!
tidy_text <- unnest_tokens(train, 'splitted', 'text', token="words") %>%
filter(!splitted %in% stop_words)
tidy_text %>% count(splitted,sort=TRUE)
# note the power functional features of R bring us!
tidy_text <- unnest_tokens(train, 'splitted', 'text', token="words") %>%
filter(!splitted %in% splitted_stop_words)
tidy_text %>% count(splitted,sort=TRUE)
tidy_text
naiveBayes <- setRefClass("naiveBayes",
# here it would be wise to have some vars to store intermediate result
# frequency dict etc. Though pay attention to bag of words!
fields = list(
class_probabilities = "list",
word_probabilities =
),
methods = list(
# function pre-processes dataset and returns a matrix.
# In matrix each row is a word and column is a class.
# Cell represents number of words in this class.
preprocess = function(data) {
words = unique(data$splitted)
classes = c("negative", "neutral", "positive")
matr = matrix(0,
nrow=length(words),
ncol=length(classes),
dimnames=list(words, classes)
)
for (cls in classes) {
class_data <- data[data$sentiment==cls, ]
word_counts <- table(class_data$splitted)
matr[names(word_counts), cls] <- as.numeric(word_counts)
}
return(mat)
},
# prepare your training data as X - bag of words for each of your
# messages and corresponding label for the message encoded as 0 or 1
# (binary classification task)
fit = function(X, y) {
# TODO
},
# return prediction for a single message
predict = function(message) {
# TODO
},
# score you test set so to get the understanding how well you model
# works.
# look at f1 score or precision and recall
# visualize them
# try how well your model generalizes to real world data!
score = function(X_test, y_test)
{
# TODO
}
))
naiveBayes <- setRefClass("naiveBayes",
# here it would be wise to have some vars to store intermediate result
# frequency dict etc. Though pay attention to bag of words!
fields = list(
class_probabilities = "list",
word_probabilities = "list"
),
methods = list(
# function pre-processes dataset and returns a matrix.
# In matrix each row is a word and column is a class.
# Cell represents number of words in this class.
preprocess = function(data) {
words = unique(data$splitted)
classes = c("negative", "neutral", "positive")
matr = matrix(0,
nrow=length(words),
ncol=length(classes),
dimnames=list(words, classes)
)
for (cls in classes) {
class_data <- data[data$sentiment==cls, ]
word_counts <- table(class_data$splitted)
matr[names(word_counts), cls] <- as.numeric(word_counts)
}
return(mat)
},
# prepare your training data as X - bag of words for each of your
# messages and corresponding label for the message encoded as 0 or 1
# (binary classification task)
fit = function(X, y) {
# TODO
},
# return prediction for a single message
predict = function(message) {
# TODO
},
# score you test set so to get the understanding how well you model
# works.
# look at f1 score or precision and recall
# visualize them
# try how well your model generalizes to real world data!
score = function(X_test, y_test)
{
# TODO
}
))
model = naiveBayes()
model$fit()
naiveBayes <- setRefClass("naiveBayes",
# here it would be wise to have some vars to store intermediate result
# frequency dict etc. Though pay attention to bag of words!
fields = list(
class_probabilities = "list",
word_probabilities = "li"
),
methods = list(
# function pre-processes dataset and returns a matrix.
# In matrix each row is a word and column is a class.
# Cell represents number of words in this class.
preprocess = function(data) {
words = unique(data$splitted)
classes = c("negative", "neutral", "positive")
matr = matrix(0,
nrow=length(words),
ncol=length(classes),
dimnames=list(words, classes)
)
for (cls in classes) {
class_data <- data[data$sentiment==cls, ]
word_counts <- table(class_data$splitted)
matr[names(word_counts), cls] <- as.numeric(word_counts)
}
return(mat)
},
# prepare your training data as X - bag of words for each of your
# messages and corresponding label for the message encoded as 0 or 1
# (binary classification task)
fit = function(X, y) {
# TODO
},
# return prediction for a single message
predict = function(message) {
# TODO
},
# score you test set so to get the understanding how well you model
# works.
# look at f1 score or precision and recall
# visualize them
# try how well your model generalizes to real world data!
score = function(X_test, y_test)
{
# TODO
}
))
naiveBayes <- setRefClass("naiveBayes",
# here it would be wise to have some vars to store intermediate result
# frequency dict etc. Though pay attention to bag of words!
fields = list(
class_probabilities = "list",
word_probabilities = "list"
),
methods = list(
# function pre-processes dataset and returns a matrix.
# In matrix each row is a word and column is a class.
# Cell represents number of words in this class.
preprocess = function(data) {
words = unique(data$splitted)
classes = c("negative", "neutral", "positive")
matr = matrix(0,
nrow=length(words),
ncol=length(classes),
dimnames=list(words, classes)
)
for (cls in classes) {
class_data <- data[data$sentiment==cls, ]
word_counts <- table(class_data$splitted)
matr[names(word_counts), cls] <- as.numeric(word_counts)
}
return(mat)
},
# prepare your training data as X - bag of words for each of your
# messages and corresponding label for the message encoded as 0 or 1
# (binary classification task)
fit = function(X, y) {
# TODO
},
# return prediction for a single message
predict = function(message) {
# TODO
},
# score you test set so to get the understanding how well you model
# works.
# look at f1 score or precision and recall
# visualize them
# try how well your model generalizes to real world data!
score = function(X_test, y_test)
{
# TODO
}
))
model = naiveBayes()
model$fit()
a <- model.preprocess(tidy_text)
a <- model$preprocess(tidy_text)
a <- model$preprocess(tidy_text)
naiveBayes <- setRefClass("naiveBayes",
# here it would be wise to have some vars to store intermediate result
# frequency dict etc. Though pay attention to bag of words!
fields = list(
class_probabilities = "list",
word_probabilities = "listmo"
),
methods = list(
# function pre-processes dataset and returns a matrix.
# In matrix each row is a word and column is a class.
# Cell represents number of words in this class.
preprocess = function(data) {
words = unique(data$splitted)
classes = c("negative", "neutral", "positive")
matr = matrix(0,
nrow=length(words),
ncol=length(classes),
dimnames=list(words, classes)
)
for (cls in classes) {
class_data <- data[data$sentiment==cls, ]
word_counts <- table(class_data$splitted)
matr[names(word_counts), cls] <- as.numeric(word_counts)
}
return(matr)
},
# prepare your training data as X - bag of words for each of your
# messages and corresponding label for the message encoded as 0 or 1
# (binary classification task)
fit = function(X, y) {
# TODO
},
# return prediction for a single message
predict = function(message) {
# TODO
},
# score you test set so to get the understanding how well you model
# works.
# look at f1 score or precision and recall
# visualize them
# try how well your model generalizes to real world data!
score = function(X_test, y_test)
{
# TODO
}
))
a <- model$preprocess(tidy_text)
naiveBayes <- setRefClass("naiveBayes",
# here it would be wise to have some vars to store intermediate result
# frequency dict etc. Though pay attention to bag of words!
fields = list(
class_probabilities = "list",
word_probabilities = "list"
),
methods = list(
# function pre-processes dataset and returns a matrix.
# In matrix each row is a word and column is a class.
# Cell represents number of words in this class.
preprocess = function(data) {
words = unique(data$splitted)
classes = c("negative", "neutral", "positive")
matr = matrix(0,
nrow=length(words),
ncol=length(classes),
dimnames=list(words, classes)
)
for (cls in classes) {
class_data <- data[data$sentiment==cls, ]
word_counts <- table(class_data$splitted)
matr[names(word_counts), cls] <- as.numeric(word_counts)
}
return(matr)
},
# prepare your training data as X - bag of words for each of your
# messages and corresponding label for the message encoded as 0 or 1
# (binary classification task)
fit = function(X, y) {
# TODO
},
# return prediction for a single message
predict = function(message) {
# TODO
},
# score you test set so to get the understanding how well you model
# works.
# look at f1 score or precision and recall
# visualize them
# try how well your model generalizes to real world data!
score = function(X_test, y_test)
{
# TODO
}
))
model = naiveBayes()
model$fit()
a <- model$preprocess(tidy_text)
a
naiveBayes <- setRefClass("naiveBayes",
# here it would be wise to have some vars to store intermediate result
# frequency dict etc. Though pay attention to bag of words!
fields = list(
class_probabilities = "list",
word_probabilities = "list"
),
methods = list(
# function pre-processes dataset and returns a matrix.
# In matrix each row is a word and column is a class.
# Cell represents number of words in this class.
preprocess = function(data) {
words = unique(data$splitted)
classes = unique(data$sentiment)
matr = matrix(0,
nrow=length(words),
ncol=length(classes),
dimnames=list(words, classes)
)
for (cls in classes) {
class_data <- data[data$sentiment==cls, ]
word_counts <- table(class_data$splitted)
matr[names(word_counts), cls] <- as.numeric(word_counts)
}
return(matr)
},
# prepare your training data as X - bag of words for each of your
# messages and corresponding label for the message encoded as 0 or 1
# (binary classification task)
fit = function(X, y) {
# TODO
},
# return prediction for a single message
predict = function(message) {
# TODO
},
# score you test set so to get the understanding how well you model
# works.
# look at f1 score or precision and recall
# visualize them
# try how well your model generalizes to real world data!
score = function(X_test, y_test)
{
# TODO
}
))
model = naiveBayes()
model$fit()

---
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Probability and Statistics

# Lab Assignment 1: Naive Bayes Classifier

## Work breakdown

-   *Zasymovych Bohdan*:
    preprocessing, fit, predict
-   *Hombosh Oleh*:
    dataset visualization
-   *Lyzenko Diana*:
    metrics calculation and their visualization

## Introduction

During the first three weeks, you learned a couple of essential notions
and theorems, and one of the most important among them is the **Bayes
theorem**.

**Naive Bayes Classifier** is a simple algorithm, which is based on
**Bayes theorem** and used for solving classification problems.
**Classification problem** is a problem in which an observation has to
be classified in one of the $n$ classes based on its similarity with
observations in each class.

It is a **probabilistic classifier**, which means it predicts based on
the probability of an observation belonging to each class. To compute
it, this algorithm uses **Bayes' formula,** that you probably already
came across in **Lesson 3:**
$$\mathsf{P}(\mathrm{class}\mid \mathrm{observation})=\frac{\mathsf{P}(\mathrm{observation}\mid\mathrm{class})\mathsf{P}(\mathrm{class})}{\mathsf{P}(\mathrm{observation})}$$

Under the strong **independence** assumption, one can calculate
$\mathsf{P}(\mathrm{observation} \mid \mathrm{class})$ as
$$\mathsf{P}(\mathrm{observation}) = \prod_{i=1}^{n} \mathsf{P}(\mathrm{feature}_i), \qquad \mathsf{P}(\mathrm{observation} \mid \mathrm{class}) = \prod_{i=1}^{n} \mathsf{P}(\mathrm{feature}_i \mid \mathrm{class}),$$
where $n$ is the total number of features describing a given observation
(*For example, if an observation is presented as a sentence, then each
word can be a feature*). Thus,
$\mathsf{P}(\mathrm{class}|\mathrm{observation})$ now can be calculated
as

$$\mathsf{P}(\mathrm{class} \mid \mathrm{\mathrm{observation}}) = \mathsf{P}(\mathrm{class})\times \prod_{i=1}^{n}\frac{\mathsf{P}(\mathrm{feature}_i\mid \mathrm{class})}{\mathsf{P}(\mathrm{feature}_i)}\tag{1}$$

All the terms on the right-hand side can be estimated as corresponding
relative frequencies using available data\

**See [*this
link*](https://www.javatpoint.com/machine-learning-naive-bayes-classifier)
for more detailed explanations & examples :) Also you can watch [*this
video*](https://youtu.be/O2L2Uv9pdDA?si=-ohkHVDuu3sLLGMq) for more
examples!**

## Data description

There are 5 datasets uploaded on the cms (data.zip)

To determine your variant, take your team number from the list of teams
on cms and take *mod 5* - this is the number of your data set.

-   **0 - authors** This data set consists of citations of three famous
    writers: Edgar Alan Poe, Mary Wollstonecraft Shelley and HP
    Lovecraft. The task with this data set is to classify a piece of
    text with the author who was more likely to write it.

-   **1 - discrimination** This data set consists of tweets that have
    discriminatory (sexism or racism) messages or of tweets that are of
    neutral mood. The task is to determine whether a given tweet has
    discriminatory mood or does not.

-   **2 - fake news** This data set contains data of American news: a
    headline and an abstract of the article. Each piece of news is
    classified as fake or credible. The task is to classify the news
    from test.csv as credible or fake.

-   **3 - sentiment** All the text messages contained in this data set
    are labeled with three sentiments: positive, neutral or negative.
    The task is to classify some text message as the one of positive
    mood, negative or neutral.

-   **4 - spam** This last data set contains SMS messages classified as
    spam or non-spam (ham in the data set). The task is to determine
    whether a given message is spam or non-spam.

Each data set consists of two files: *train.csv* and *test.csv*. The
first one is used to find the probabilities of the corresponding classes
and the second one is used to test your classifier afterwards. Note that
it is crucial to randomly split your data into training and testing
parts to test the classifierʼs possibilities on the unseen data.

```{r}
# here goes a list of recommended libraries,
# though you may install other ones if they are needed
library(tidytext)
library(readr)
library(dplyr)
library(ggplot2)
library(tidyr) # For reshaping data (convert metrics table for easier plotting)
library(caret) # For createDataPartition
```

## Outline of the work

1.  **Data pre-processing** (includes removing punctuation marks and
    stop words, representing each message as a bag-of-words)
2.  **Data visualization** (it's time to plot your data!)
3.  **Classifier implementation** (using the training set, calculate all
    the conditional probabilities in formula (1))
4.  **Measurements of effectiveness of your classifier** (use the
    results from the previous step to predict classes for messages in
    the testing set and measure the accuracy, precision and recall, F1
    score metric etc)
5.  **Conclusions**

*!! do not forget to submit both the (compiled) Rmd source file and the
.html output !!*

## Data pre-processing

-   Read the *.csv* data files.
-   Сlear your data from punctuation or other unneeded symbols.
-   Clear you data from stop words. You don't want words as is, and, or
    etc. to affect your probabilities distributions, so it is a wise
    decision to get rid of them. Find list of stop words in the cms
    under the lab task.
-   Represent each test message as its bag-of-words. Here:
    <https://machinelearningmastery.com/gentle-introduction-bag-words-model/>
    you can find general introduction to the bag-of-words model and
    examples on to create it.
-   It is highly recommended to get familiar with R dataframes, it would
    make the work much easier to do.
-   Useful links:
    -   <https://steviep42.github.io/webscraping/book/bagofwords.html#tidytext> -
        example of using *tidytext* to count frequencies of the words.
    -   Basics of Text Mining in R:
        <http://rstudio-pubs-static.s3.amazonaws.com/256588_57b585da6c054349825cba46685d8464.html>
        . Note that it also includes an example on how to create a bag
        of words from your text document.

```{r}
list.files(getwd())
list.files("data/3-sentiment")
```

```{r}
test_path <- "data/3-sentiment/test.csv"
train_path <- "data/3-sentiment/train.csv"

stop_words <- read_file("stop_words.txt")
# https://stackoverflow.com/questions/27195912/why-does-strsplit-return-a-list
splitted_stop_words <- strsplit(stop_words, split='\n')
splitted_stop_words <- splitted_stop_words[[1]]
```

```{r}
train <-  read.csv(file = train_path, stringsAsFactors = FALSE)
test <-  read.csv(file = test_path, stringsAsFactors = FALSE)
```

```{r}
# note the power functional features of R bring us! 
tidy_text <- unnest_tokens(train, 'splitted', 'text', token="words") %>%
             filter(!splitted %in% splitted_stop_words)

tidy_text_test <- unnest_tokens(test, 'splitted', 'text', token="words") %>%
             filter(!splitted %in% splitted_stop_words)
```

## Data visualization

Each time you work with some data, you need to understand it before you
start processing it. R has very powerful tools to make nice plots and
visualization. Show what are the most common words for negative and
positive examples as a histogram, word cloud etc. Be creative!

### Histograms of the most frequent words for each class

For each class (negative, positive, neutral), the 10 most frequent words from the cleaned corpus (after removing stop words) are selected. The data is filtered by class, sorted by frequency, and a horizontal histogram (barplot) is constructed, where the X axis is the word, and the Y axis is the number of its occurrences in the class. This allows you to quickly see which words are most characteristic of each type of message.

```{r}
top_neg <- tidy_text %>%
  dplyr::filter(sentiment == "negative") %>%
  dplyr::count(splitted, sort = TRUE) %>%
  dplyr::slice_max(n, n = 10, with_ties = FALSE) %>%
  dplyr::arrange(n) %>%
  dplyr::mutate(splitted = factor(splitted, levels = splitted))

top_pos <- tidy_text %>%
  dplyr::filter(sentiment == "positive") %>%
  dplyr::count(splitted, sort = TRUE) %>%
  dplyr::slice_max(n, n = 10, with_ties = FALSE) %>%
  dplyr::arrange(n) %>%
  dplyr::mutate(splitted = factor(splitted, levels = splitted))

top_neu <- tidy_text %>%
  dplyr::filter(sentiment == "neutral") %>%
  dplyr::count(splitted, sort = TRUE) %>%
  dplyr::slice_max(n, n = 10, with_ties = FALSE) %>%
  dplyr::arrange(n) %>%
  dplyr::mutate(splitted = factor(splitted, levels = splitted))

plot_neg <- ggplot(top_neg, aes(x = splitted, y = n, fill = "negative")) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(title = "Top-10 words — Negative", x = "word", y = "count")
plot_neg

plot_pos <- ggplot(top_pos, aes(x = splitted, y = n, fill = "positive")) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(title = "Top-10 words — Positive", x = "word", y = "count")
plot_pos

plot_neu <- ggplot(top_neu, aes(x = splitted, y = n, fill = "neutral")) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(title = "Top-10 words — Neutral", x = "word", y = "count")
plot_neu
```

### Class balance

A simple bar chart is constructed to show whether there is an imbalance between classes. This is important for understanding how class imbalance can affect classification results.

```{r}
class_balance <- train %>%
  dplyr::count(sentiment)

ggplot(class_balance, aes(x = sentiment, y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  labs(title = "Class balance", x = "word", y = "count")
```

### n-gram frequencies

Three-grams (sequences of three words) are formed from the text for each class. The 15 most frequent trigrams are selected for each class.

```{r}
bigrams <- train %>%
  dplyr::select(sentiment, text) %>%
  tidytext::unnest_ngrams(bigram, text, n = 3)

top_bigram <- bigrams %>%
  dplyr::count(sentiment, bigram, sort = TRUE) %>%
  dplyr::group_by(sentiment) %>%
  dplyr::slice_max(n, n = 15, with_ties = FALSE) %>%
  dplyr::ungroup()

neg_bigram <- top_bigram %>%
  dplyr::filter(sentiment == "negative") %>%
  dplyr::arrange(n) %>%
  dplyr::mutate(bigram = factor(bigram, levels = bigram))

plot_neg_bigram <- ggplot(neg_bigram, aes(x = bigram, y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(title = "Top-15 three-gram — Negative", x = "three-gram", y = "count")
plot_neg_bigram

pos_bigram <- top_bigram %>%
  dplyr::filter(sentiment == "positive") %>%
  dplyr::arrange(n) %>%
  dplyr::mutate(bigram = factor(bigram, levels = bigram))

plot_pos_bigram <- ggplot(pos_bigram, aes(x = bigram, y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(title = "Top-15 three-gram — Positive", x = "three-gram", y = "count")
plot_pos_bigram

neu_bigram <- top_bigram %>%
  dplyr::filter(sentiment == "neutral") %>%
  dplyr::arrange(n) %>%
  dplyr::mutate(bigram = factor(bigram, levels = bigram))

plot_neu_bigram <- ggplot(neu_bigram, aes(x = bigram, y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(title = "Top-15 three-gram — Neutral", x = "three-gram", y = "count")
plot_neu_bigram
```

### Common and unique words

For each class, the top 50 most frequent words are selected. The following are determined: words that are in the top 50 for both positive and negative ("shared pos-neg"), words that are only in positive ("unique pos"), only in negative ("unique neg"), only in neutral ("unique neu"). A bar chart is constructed that shows how many words are common and how many are unique for each class.

```{r}
freq_all <- tidy_text %>%
  dplyr::count(sentiment, splitted, sort = TRUE)

pos_words <- freq_all %>% dplyr::filter(sentiment == "positive") %>% dplyr::pull(splitted)
neg_words <- freq_all %>% dplyr::filter(sentiment == "negative") %>% dplyr::pull(splitted)
neu_words <- freq_all %>% dplyr::filter(sentiment == "neutral") %>% dplyr::pull(splitted)

shared_pos_neg <- intersect(pos_words, neg_words)
unique_pos <- setdiff(pos_words, union(neg_words, neu_words))
unique_neg <- setdiff(neg_words, union(pos_words, neu_words))
unique_neu <- setdiff(neu_words, union(pos_words, neg_words))

sets_df <- tibble::tibble(
  set = c("shared pos-neg", "unique pos", "unique neg", "unique neu"),
  size = c(length(shared_pos_neg), length(unique_pos), length(unique_neg), length(unique_neu))
)

ggplot(sets_df, aes(x = set, y = size, fill = set)) +
  geom_col(show.legend = FALSE) +
  labs(title = "Common and unique top-words", x = "category", y = "word count")
```


### Cleansing and its effect

The top 15 most frequent words before cleaning (on raw tokens) and after cleaning (after removing stop words) are compared.
```{r}
freq_raw <- train %>%
  dplyr::select(sentiment, text) %>%
  tidytext::unnest_tokens(word, text) %>%
  dplyr::count(word, sort = TRUE) %>%
  dplyr::slice_max(n, n = 15, with_ties = FALSE) %>%
  dplyr::mutate(word = factor(word, levels = rev(word)))

freq_clean <- tidy_text %>%
  dplyr::count(splitted, sort = TRUE) %>%
  dplyr::slice_max(n, n = 15, with_ties = FALSE) %>%
  dplyr::mutate(splitted = factor(splitted, levels = rev(splitted)))

p_raw <- ggplot(freq_raw, aes(x = word, y = n)) +
  geom_col(fill = "gray50") +
  coord_flip() +
  labs(title = "Before cleaning (top-15)", x = "word", y = "count")
p_raw

p_clean <- ggplot(freq_clean, aes(x = splitted, y = n)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "After cleaning (top-15)", x = "word", y = "count")
p_clean
```

## Classifier implementation

```{r}
naiveBayes <- setRefClass("naiveBayes",
                           
       # here it would be wise to have some vars to store intermediate result
       # frequency dict etc. Though pay attention to bag of words! 
       fields = list(
                    # Named vector with probabilities for each class
                    class_probabilities = "numeric",
                    
                    # Matrix where row is a word and column is a class.
                    # Cell represents probability of a word for a class.
                    word_probabilities = "matrix",
                    
                    # Alpha parameter for Laplace smoothing
                    alpha = "numeric"
                    ),
       
       methods = list(
                    initialize = function(alpha_val=1, ...) { # Initialize default value for alpha
                      alpha <<- alpha_val
                      callSuper(...)
                    },
                    
                    
                    # function pre-processes dataset and returns a matrix.
                    # In matrix each row is a word and column is a class.
                    # Cell represents number of words in this class.
                    preprocess = function(data) {
                      words = unique(data$splitted)
                      classes = c("negative", "neutral", "positive")
                      
                      freq_matr = matrix(0,
                                    nrow=length(words),
                                    ncol=length(classes),
                                    dimnames=list(words, classes)
                                  )
                        
                      for (cls in classes) {
                          class_data <- data[data$sentiment==cls, ]
                          word_counts <- table(class_data$splitted)
                          freq_matr[names(word_counts), cls] <- as.numeric(word_counts)
                      }
                      
                      return(freq_matr)
                    },
                    
                    
                    # Method sets values to class_probabilities and word_probabilities attributes.
                    # X is tidy_text, dataframe where each row is a word and column sentiment is class, column splitted is a word
                    fit = function(X) {
                      freq_matr = preprocess(X) + alpha # Apply Laplace smoothing
                      total_words_per_class <- colSums(freq_matr)
                      word_probabilities <<- sweep(freq_matr, 2, total_words_per_class, FUN="/")

                      class_count <- table(X$sentiment)
                      class_probabilities <<- as.numeric(class_count) / nrow(X)
                      names(class_probabilities) <<- names(class_count)
                    },
                  
                    
                    # Returns prediction for a single message.
                    # For sentiment dataset prediction is "negative", "neutral" or "positive"
                    predict = function(message) {
                      message_df <- data.frame(id=1, text=message, stringsAsFactors=FALSE)
                      message_words <- message_df %>%
                        unnest_tokens(word, text, token="words") %>%
                        filter(!word %in% splitted_stop_words)
                      message_words <- message_words$word[message_words$word %in% rownames(word_probabilities)]

                      if (length(message_words) == 0) {
                        return("neutral")
                      }

                      log_probs <- colSums(log(word_probabilities[message_words, , drop=FALSE]))
                      log_probs <- log_probs + log(class_probabilities)
                      prediction <- names(log_probs)[which.max(log_probs)]

                      return(prediction)
                    },

                    score = function(X_test, y_test) {
                      predictions <- sapply(X_test$text, function(massage) predict(massage))
                      classes <- unique(y_test)
                      metrics <- data.frame()
                      for (cls in classes) {
                        TP <- sum(predictions == cls & y_test == cls)
                        FP <- sum(predictions == cls & y_test != cls)
                        FN <- sum(predictions != cls & y_test == cls)
                        precision <- if (TP + FP > 0) TP / (TP + FP) else 0
                        recall <- if (TP + FN > 0) TP / (TP + FN) else 0
                        f1 <- if (precision + recall > 0) 2 * (precision * recall / (precision + recall)) else 0
                        metrics <- rbind(metrics, data.frame(class=cls, precision=precision, recall=recall, f1=f1)) }
                      return(metrics)
                      }
       )
)
```

## Measure effectiveness of your classifier

-   Note that accuracy is not always a good metric for your classifier.
    Look at precision and recall curves, F1 score metric.

    When evaluating the model, it's important to understand the
    different types of classification results:

    -   A ***true positive*** result is one where the model correctly
        predicts the positive class.
    -   A ***true negative*** result is one where the model correctly
        predicts the negative class.
    -   A ***false positive*** result is one where the model incorrectly
        predicts the positive class when it is actually negative.
    -   A ***false negative*** result is one where the model incorrectly
        predicts the negative class when it is actually positive.

    Precision measures the proportion of true positive predictions among
    all positive predictions made by the model.

    $$Precision = \frac{TP}{TP+FP}$$

    Recall, on the other hand, measures the proportion of true positives
    identified out of all actual positive cases.

    $$Recall = \frac{TP}{TP+FN}$$

    F1 score is the harmonic mean of both precision and recall.

    $$F1 = \frac{2\times Precision \times Recall}{Precision + Recall}$$

    **See [this
    link](https://cohere.com/blog/classification-eval-metrics) to find
    more information about metrics.**

-   Visualize them.

-   Show failure cases.

```{r}
model <- naiveBayes()
model$fit(tidy_text)
X_test <- test
y_test <- test$sentiment
metrics <- model$score(X_test, y_test)
print(metrics)

predictions <- sapply(X_test$text, function(msg) model$predict(msg))

metrics_long <- metrics %>% pivot_longer(cols=c(precision, recall, f1),
                                         names_to="metric", values_to="value")
metrics_long$metric <- factor(metrics_long$metric, levels = c("precision", "recall", "f1"))

# Plot precision, recall, and f1 per class
ggplot(metrics_long, aes(x=class, y=value, fill=metric)) +
  geom_bar(stat="identity", position="dodge") +
  labs(title="CModel Performance by Class")

# Predictions
predictions <- sapply(X_test$text, function(msg) model$predict(msg))

# Failure cases
failures <- data.frame(
  predicted = predictions[predictions != y_test],
  real      = y_test[predictions != y_test],
  message   = X_test$text[predictions != y_test],
  stringsAsFactors = FALSE
)
# 10 wrong predictions
head(failures, 10)

# Confusion matrix
conf_matrix <- table(True = y_test, Predicted = predictions)
conf_df <- as.data.frame(conf_matrix)
colnames(conf_df) <- c("True", "Predicted", "Count")

ggplot(conf_df, aes(x = True, y = Predicted, fill = Count)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Count), color = "black", size = 5) +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(title = "Confusion Matrix", x = "True Label", y = "Predicted Label") +
  theme_minimal()

# Weighted F1
class_counts <- table(y_test)
metrics$f1_weighted <- metrics$f1 * class_counts[metrics$class]
weighted_f1 <- sum(metrics$f1_weighted) / sum(class_counts)
cat("Weighted F1 score: ", weighted_f1, "\n")
```

## Comparison of train and test sets
```{r}
class_counts <- as.data.frame(table(tidy_text$sentiment))
colnames(class_counts) <- c("Sentiment", "Count")

ggplot(class_counts, aes(x = Sentiment, y = Count, fill = Sentiment)) +
  geom_bar(stat = "identity") +
  labs(title = "Class Distribution in Training Set", x = "Sentiment", y = "Token Count") +
  theme_minimal() +
  scale_fill_manual(values = c("negative" = "red", "neutral" = "gray", "positive" = "green"))
```

```{r}
class_counts <- as.data.frame(table(tidy_text_test$sentiment))
colnames(class_counts) <- c("Sentiment", "Count")

ggplot(class_counts, aes(x = Sentiment, y = Count, fill = Sentiment)) +
  geom_bar(stat = "identity") +
  labs(title = "Class Distribution in Testing Set", x = "Sentiment", y = "Token Count") +
  theme_minimal() +
  scale_fill_manual(values = c("negative" = "red", "neutral" = "gray", "positive" = "green"))
```

The train and test splits are highly imbalanced because the class distributions differ significantly between them. As a result, the model performs poorly since it is trained on one distribution but evaluated on another.

To fix this, we combined the two sets and then split them again with preservation of class balance.

```{r}
# Combine sets
all_data <- bind_rows(train, test)

# Split the data
set.seed(123)
train_index <- createDataPartition(all_data$sentiment, p = 0.8, list = FALSE)
train_balanced <- all_data[train_index, ]
test_balanced  <- all_data[-train_index, ]

# Check the distribution
table(train_balanced$sentiment)
table(test_balanced$sentiment)
```

```{r}
# Clear data
tidy_text_balanced <- unnest_tokens(train_balanced, 'splitted', 'text', token="words") %>%
             filter(!splitted %in% splitted_stop_words)

tidy_text_test_balanced <- unnest_tokens(test_balanced, 'splitted', 'text', token="words") %>%
             filter(!splitted %in% splitted_stop_words)
```

```{r}
class_counts <- as.data.frame(table(tidy_text_balanced$sentiment))
colnames(class_counts) <- c("Sentiment", "Count")

# bar plot
ggplot(class_counts, aes(x = Sentiment, y = Count, fill = Sentiment)) +
  geom_bar(stat = "identity") +
  labs(title = "Class Distribution in New Balanced Training Set", x = "Sentiment", y = "Token Count") +
  theme_minimal() +
  scale_fill_manual(values = c("negative" = "red", "neutral" = "gray", "positive" = "green"))
```

```{r}
class_counts <- as.data.frame(table(tidy_text_test_balanced$sentiment))
colnames(class_counts) <- c("Sentiment", "Count")

# bar plot
ggplot(class_counts, aes(x = Sentiment, y = Count, fill = Sentiment)) +
  geom_bar(stat = "identity") +
  labs(title = "Class Distribution in New Balanced Test Set", x = "Sentiment", y = "Token Count") +
  theme_minimal() +
  scale_fill_manual(values = c("negative" = "red", "neutral" = "gray", "positive" = "green"))
```

```{r}
model = naiveBayes()
model$fit(tidy_text_balanced)
X_test <- test_balanced
y_test <- test_balanced$sentiment
metrics <- model$score(X_test, y_test)
print(metrics)

predictions <- sapply(X_test$text, function(msg) model$predict(msg))

metrics_long <- metrics %>% pivot_longer(cols=c(precision, recall, f1), names_to="metric", values_to="value")
metrics_long$metric <- factor(metrics_long$metric, levels = c("precision", "recall", "f1"))

# Plot precision, recall, and f1 for each class
ggplot(metrics_long, aes(x=class, y=value, fill=metric)) +
  geom_bar(stat="identity", position="dodge") +
  labs(title="CModel Performance by Class")

# Failure cases
failures <- data.frame(
  predicted = predictions[predictions != y_test],
  real      = y_test[predictions != y_test],
  message   = X_test$text[predictions != y_test],
  stringsAsFactors = FALSE
)
# 10 wrong predictions
head(failures, 10)

# Build confusion matrix
conf_matrix <- table(True = y_test, Predicted = predictions)
conf_df <- as.data.frame(conf_matrix)
colnames(conf_df) <- c("True", "Predicted", "Count")
ggplot(conf_df, aes(x = True, y = Predicted, fill = Count)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Count), color = "black", size = 5) +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(title = "Confusion Matrix", x = "True Label", y = "Predicted Label") +
  theme_minimal()

# Calculate weighted F1
class_counts <- table(y_test)
metrics$f1_weighted <- metrics$f1 * class_counts[metrics$class]
weighted_f1 <- sum(metrics$f1_weighted) / sum(class_counts)

cat("Weighted F1 score: ", weighted_f1, "\n")
```

## Conclusions

Summarize your work by explaining in a few sentences the points listed
below.

-   Describe the method implemented in general. Show what are
    mathematical foundations you are basing your solution on.
-   List pros and cons of the method. This should include the
    limitations of your method, all the assumption you make about the
    nature of your data etc.
-   Explain why accuracy is not a good choice for the base metrics for
    classification tasks. Why F1 score is always preferable?


### Data preprocesing
  - Loading and cleaning – The raw CSV files are read into R, punctuation or other unnecessary symbols are removed to reduce noise.
  - Tokenization – Each message is split into individual words (tokens), transforming text into a structured form suitable for frequency counting.
  - Stop-word removal – Common words that do not carry any information for classification are filtered out using a predefined stop-word list.
  - Data representation – Each token is represented in a frequency matrix where rows are words and columns are classes. This matrix counts occurrences of each word in each class.
  
### Classification Method
The classification model is based on **Bayes’ theorem**, which allows computing the probability of a class given an observation. For text classification, the probability of a message belonging to a class is estimated as the product of the probabilities of its words belonging to that class. To handle words that are absent in a class, **Laplace smoothing** is applied during the `fit` step: 1 is added to each word count. This prevents a single unseen word from incorrectly assigning zero probability to a class. During the `predict` step, unknown words are simply ignored, as there is no information about them in the training data. Logarithms are applied to prevent numerical errors that can occur when multiplying many very small probabilities. Since the product of numerous values close to zero can exceed the precision limits of a computer, taking the logarithm of probabilities allows summing them instead, which is more stable and avoids underflow issues. (But for our dataset it appeared unnecessary and didn't change anything)

**Pros:**
- Simple and computationally efficient.
- Performs well on high-dimensional data such as text.

**Limitations:**
- Does not capture word order in sentences.
- Sensitive to class imbalance in the training data.
- Assumes conditional independence of words, which is rarely true in practice since the occurrence of one word often affects the probability of the others.

### Metrics and Performnace
To estimate performance of our model we've used **precision**, **recall**, **F1 score**. To get better insight into how does model confuses classes **confussion matrix** was created. To evaluate joint performance of a model over all classes **weighted F1 score** was used. Weighed F1 score was used instead of average to account unbalanced distribution of the classes.
$$
\text{F1}_{\text{weighted}} = \sum_{i=1}^{C} p_i \cdot \text{F1}_i \newline \newline p_i - \text{number of elements in a class i over the total number of elements} \newline F1_i - \text{F1 score for class i} 
$$

### Why accuracy is insufficient and F1 score is preferable:
Accuracy can be misleading, especially with imbalanced datasets (where one class dominates). For example, in a sentiment analysis task where 90% of messages are neutral and only 10% are positive or negative, a classifier could achieve 90% accuracy by always predicting "neutral," while completely failing to identify positive or negative messages. The F1 score provides a better measure by balancing precision (how many selected items are relevant) and recall (how many relevant items are selected), making it more robust for evaluating classification performance across all classes.

### Sets imbalance
We have checked the number of messages in each class for both training and test sets and saw that the distributions were very different. Some classes had many more examples than others, which caused the model to misclassify minority classes. To fix this, we created a balanced split that preserved the class proportions in train and test. After doing this, the model’s predictions became more accurate, especially weighted F1 score increased from 0.26 to 0.72.
